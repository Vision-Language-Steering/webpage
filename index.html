<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="description" content="VLS: Steering Pretrained Robot Policies via Vision-Language Models - A training-free framework for inference-time adaptation of frozen generative robot policies.">
  <meta name="keywords" content="VLS, Vision-Language, Robot Manipulation, Diffusion Policy, Steering, Inference-Time Adaptation, Robotics">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>VLS: Steering Pretrained Robot Policies via Vision–Language Models</title>

  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <style>
    :root {
      --primary: #2563eb;
      --primary-dark: #1d4ed8;
      --secondary: #7c3aed;
      --accent: #06b6d4;
      --success: #10b981;
      --warning: #f59e0b;
      --bg-light: #f8fafc;
      --bg-dark: #0f172a;
      --text-primary: #1e293b;
      --text-secondary: #64748b;
      --border: #e2e8f0;
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
      line-height: 1.7;
      color: var(--text-primary);
      background: #fff;
    }

    .container {
      max-width: 1100px;
      margin: 0 auto;
      padding: 0 24px;
    }

    /* Hero Section */
    .hero {
      background: linear-gradient(135deg, #1e3a5f 0%, #0f172a 50%, #1a1a2e 100%);
      color: white;
      padding: 80px 0 60px;
      position: relative;
      overflow: hidden;
    }

    .hero::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      right: 0;
      bottom: 0;
      background: url("data:image/svg+xml,%3Csvg width='60' height='60' viewBox='0 0 60 60' xmlns='http://www.w3.org/2000/svg'%3E%3Cg fill='none' fill-rule='evenodd'%3E%3Cg fill='%239C92AC' fill-opacity='0.05'%3E%3Cpath d='M36 34v-4h-2v4h-4v2h4v4h2v-4h4v-2h-4zm0-30V0h-2v4h-4v2h4v4h2V6h4V4h-4zM6 34v-4H4v4H0v2h4v4h2v-4h4v-2H6zM6 4V0H4v4H0v2h4v4h2V6h4V4H6z'/%3E%3C/g%3E%3C/g%3E%3C/svg%3E");
    }

    .hero-content {
      position: relative;
      z-index: 1;
      text-align: center;
    }

    .venue-badge {
      display: inline-block;
      background: rgba(255,255,255,0.1);
      border: 1px solid rgba(255,255,255,0.2);
      padding: 6px 16px;
      border-radius: 20px;
      font-size: 0.85rem;
      font-weight: 500;
      margin-bottom: 24px;
      backdrop-filter: blur(10px);
    }

    .hero h1 {
      font-size: 2.75rem;
      font-weight: 700;
      line-height: 1.2;
      margin-bottom: 20px;
      letter-spacing: -0.02em;
    }

    .hero h1 .highlight {
      background: linear-gradient(90deg, var(--accent), var(--primary));
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    .authors {
      font-size: 1.1rem;
      color: rgba(255,255,255,0.8);
      margin-bottom: 32px;
    }

    .btn-group {
      display: flex;
      gap: 12px;
      justify-content: center;
      flex-wrap: wrap;
    }

    .btn {
      display: inline-flex;
      align-items: center;
      gap: 8px;
      padding: 12px 24px;
      border-radius: 8px;
      font-weight: 500;
      font-size: 0.95rem;
      text-decoration: none;
      transition: all 0.2s;
    }

    .btn-primary {
      background: white;
      color: var(--bg-dark);
    }

    .btn-primary:hover {
      background: #f1f5f9;
      transform: translateY(-2px);
    }

    .btn-outline {
      background: transparent;
      color: white;
      border: 1px solid rgba(255,255,255,0.3);
    }

    .btn-outline:hover {
      background: rgba(255,255,255,0.1);
      border-color: rgba(255,255,255,0.5);
    }

    /* Teaser */
    .teaser {
      padding: 60px 0 40px;
      background: var(--bg-light);
    }

    .teaser-img {
      width: 100%;
      border-radius: 12px;
      box-shadow: 0 20px 60px rgba(0,0,0,0.1);
    }

    .teaser-caption {
      max-width: 900px;
      margin: 24px auto 0;
      text-align: center;
      color: var(--text-secondary);
      font-size: 1rem;
      line-height: 1.8;
    }

    .teaser-caption b {
      color: var(--text-primary);
    }

    /* Section Styles */
    section {
      padding: 70px 0;
    }

    section.alt-bg {
      background: var(--bg-light);
    }

    .section-title {
      font-size: 2rem;
      font-weight: 700;
      margin-bottom: 24px;
      color: var(--text-primary);
      display: flex;
      align-items: center;
      gap: 12px;
    }

    .section-title .icon {
      width: 40px;
      height: 40px;
      background: linear-gradient(135deg, var(--primary), var(--secondary));
      border-radius: 10px;
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
      font-size: 1.1rem;
    }

    .prose {
      font-size: 1.05rem;
      color: var(--text-secondary);
      line-height: 1.8;
    }

    .prose p {
      margin-bottom: 16px;
    }

    .prose strong {
      color: var(--text-primary);
      font-weight: 600;
    }

    /* Key Stats */
    .stats-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 20px;
      margin: 40px 0;
    }

    .stat-card {
      background: white;
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 24px;
      text-align: center;
      transition: all 0.2s;
    }

    .stat-card:hover {
      border-color: var(--primary);
      box-shadow: 0 4px 20px rgba(37, 99, 235, 0.1);
    }

    .stat-value {
      font-size: 2.5rem;
      font-weight: 700;
      color: var(--primary);
      line-height: 1;
    }

    .stat-label {
      font-size: 0.9rem;
      color: var(--text-secondary);
      margin-top: 8px;
    }

    /* Method Components */
    .method-figure {
      margin: 30px 0;
    }

    .method-figure img {
      width: 100%;
      border-radius: 12px;
      border: 1px solid var(--border);
    }

    .method-figure figcaption {
      margin-top: 16px;
      font-size: 0.95rem;
      color: var(--text-secondary);
      text-align: center;
    }

    .components-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
      gap: 24px;
      margin-top: 40px;
    }

    .component-card {
      background: white;
      border: 1px solid var(--border);
      border-radius: 12px;
      padding: 28px;
      transition: all 0.2s;
    }

    .component-card:hover {
      border-color: var(--primary);
      transform: translateY(-4px);
      box-shadow: 0 12px 40px rgba(0,0,0,0.08);
    }

    .component-num {
      width: 32px;
      height: 32px;
      background: linear-gradient(135deg, var(--primary), var(--secondary));
      color: white;
      border-radius: 8px;
      display: flex;
      align-items: center;
      justify-content: center;
      font-weight: 600;
      font-size: 0.9rem;
      margin-bottom: 16px;
    }

    .component-card h4 {
      font-size: 1.1rem;
      font-weight: 600;
      margin-bottom: 12px;
      color: var(--text-primary);
    }

    .component-card p {
      font-size: 0.95rem;
      color: var(--text-secondary);
      line-height: 1.7;
    }

    /* Results Tables */
    .results-table-container {
      overflow-x: auto;
      margin: 30px 0;
    }

    .results-table {
      width: 100%;
      border-collapse: collapse;
      font-size: 0.9rem;
    }

    .results-table th,
    .results-table td {
      padding: 14px 16px;
      text-align: center;
      border-bottom: 1px solid var(--border);
    }

    .results-table th {
      background: var(--bg-light);
      font-weight: 600;
      color: var(--text-primary);
    }

    .results-table tr:hover {
      background: #f8fafc;
    }

    .results-table .highlight-row {
      background: linear-gradient(90deg, rgba(16, 185, 129, 0.08), rgba(6, 182, 212, 0.08));
      font-weight: 600;
    }

    .results-table .highlight-row td {
      color: var(--text-primary);
    }

    .results-table .method-name {
      text-align: left;
      font-weight: 500;
    }

    /* Figure Grid */
    .figure-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(400px, 1fr));
      gap: 30px;
      margin: 30px 0;
    }

    .figure-item {
      background: white;
      border: 1px solid var(--border);
      border-radius: 12px;
      overflow: hidden;
    }

    .figure-item img {
      width: 100%;
      display: block;
    }

    .figure-item figcaption {
      padding: 16px 20px;
      font-size: 0.9rem;
      color: var(--text-secondary);
      background: var(--bg-light);
    }

    .figure-item figcaption strong {
      color: var(--text-primary);
    }

    .full-figure {
      margin: 30px 0;
    }

    .full-figure img {
      width: 100%;
      border-radius: 12px;
      border: 1px solid var(--border);
    }

    .full-figure figcaption {
      margin-top: 16px;
      font-size: 0.95rem;
      color: var(--text-secondary);
      line-height: 1.7;
    }

    /* Video Sections */
    .video-section-title {
      font-size: 1.25rem;
      font-weight: 600;
      margin: 40px 0 20px;
      padding-bottom: 12px;
      border-bottom: 2px solid var(--primary);
      display: inline-block;
    }

    .video-grid {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));
      gap: 20px;
    }

    .video-card {
      background: white;
      border: 1px solid var(--border);
      border-radius: 12px;
      overflow: hidden;
      transition: all 0.2s;
    }

    .video-card:hover {
      border-color: var(--primary);
      box-shadow: 0 8px 30px rgba(0,0,0,0.08);
    }

    .video-card video {
      width: 100%;
      display: block;
    }

    .video-card .video-label {
      padding: 12px 16px;
      font-size: 0.9rem;
      font-weight: 500;
      color: var(--text-primary);
      background: var(--bg-light);
      text-align: center;
    }

    .video-placeholder {
      aspect-ratio: 16/10;
      background: linear-gradient(135deg, #f1f5f9, #e2e8f0);
      display: flex;
      align-items: center;
      justify-content: center;
      color: var(--text-secondary);
      font-size: 0.9rem;
      font-style: italic;
    }

    /* Key Findings */
    .findings-list {
      list-style: none;
      margin: 24px 0;
    }

    .findings-list li {
      padding: 16px 20px;
      background: white;
      border: 1px solid var(--border);
      border-radius: 10px;
      margin-bottom: 12px;
      display: flex;
      gap: 14px;
      align-items: flex-start;
    }

    .findings-list li::before {
      content: '✓';
      width: 24px;
      height: 24px;
      background: var(--success);
      color: white;
      border-radius: 50%;
      display: flex;
      align-items: center;
      justify-content: center;
      font-size: 0.75rem;
      flex-shrink: 0;
      margin-top: 2px;
    }

    .findings-list strong {
      color: var(--text-primary);
    }

    /* BibTeX */
    .bibtex-box {
      background: var(--bg-dark);
      border-radius: 12px;
      padding: 24px;
      overflow-x: auto;
    }

    .bibtex-box pre {
      color: #e2e8f0;
      font-family: 'JetBrains Mono', monospace;
      font-size: 0.85rem;
      line-height: 1.6;
      margin: 0;
    }

    /* Footer */
    footer {
      background: var(--bg-dark);
      color: rgba(255,255,255,0.7);
      padding: 40px 0;
      text-align: center;
      font-size: 0.9rem;
    }

    footer a {
      color: var(--accent);
      text-decoration: none;
    }

    /* Responsive */
    @media (max-width: 768px) {
      .hero h1 {
        font-size: 1.8rem;
      }

      .section-title {
        font-size: 1.5rem;
      }

      .stat-value {
        font-size: 2rem;
      }

      .figure-grid {
        grid-template-columns: 1fr;
      }

      .video-grid {
        grid-template-columns: repeat(auto-fill, minmax(240px, 1fr));
      }
    }
  </style>
</head>
<body>

<!-- Hero Section -->
<section class="hero">
  <div class="container">
    <div class="hero-content">
      <h1><span class="highlight">VLS</span>: Steering Pretrained Robot Policies<br>via Vision–Language Models</h1>
      <p class="authors">
        <a href="#" style="color: inherit; text-decoration: none;">Shuo Liu</a><sup>1,4</sup>,
        <a href="#" style="color: inherit; text-decoration: none;">Ishneet Sukhvinder Singh</a><sup>2</sup>,
        <a href="https://eeching.github.io/" style="color: inherit; text-decoration: none;">Yiqing Xu</a><sup>3,4</sup>,
        <a href="https://duanjiafei.com/" style="color: inherit; text-decoration: none;">Jiafei Duan</a><sup>1,4</sup><sup>*</sup>,
        <a href="https://www.ranjaykrishna.com/index.html" style="color: inherit; text-decoration: none;">Ranjay Krishna</a><sup>1,4</sup><sup>*</sup>
      </p>
      <p style="font-size: 0.9rem; color: rgba(255,255,255,0.7); margin-bottom: 20px;">
        University of Washington<sup>1</sup> &nbsp;&nbsp;|&nbsp;&nbsp;
        University of Oxford<sup>2</sup> &nbsp;&nbsp;|&nbsp;&nbsp;
        National University of Singapore<sup>3</sup> &nbsp;&nbsp;|&nbsp;&nbsp;
        Allen Institute for AI<sup>4</sup>
      </p>
      <p style="font-size: 0.9rem; color: rgba(255,255,255,0.9); margin-bottom: 20px;">
        <sup>*</sup>Co-advised
      </p>
      <div class="btn-group">
        <a href="./VLS.pdf" class="btn btn-primary">
          <i class="fas fa-file-pdf"></i> Paper
        </a>
        <a href="#" class="btn btn-outline" style="opacity: 0.6; cursor: not-allowed;" onclick="return false;">
          <i class="ai ai-arxiv"></i> arXiv (Coming Soon)
        </a>
        <a href="#" class="btn btn-outline" style="opacity: 0.6; cursor: not-allowed;" onclick="return false;">
          <i class="fab fa-github"></i> Code (Coming Soon)
        </a>
      </div>
    </div>
  </div>
</section>

<!-- Teaser -->
<section class="teaser">
  <div class="container">
    <img src="./static/images/Figure1.png" alt="VLS Overview" class="teaser-img">
    <p class="teaser-caption">
      <b>Vision–Language Steering (VLS)</b> is a training-free framework for inference-time steering of frozen generative robot policies.
      By leveraging VLMs to generate differentiable reward functions for partially denoised action proposals, VLS enables pretrained
      diffusion or flow-matching policies to adapt to out-of-distribution scenarios—such as object changes, scene changes, or instruction
      changes—without any fine-tuning.
    </p>
  </div>
</section>

<!-- Video Overview -->
<section class="teaser" style="background: #fff;">
  <div class="container">
    <video width="100%" controls playsinline>
      <source src="./static/videos/overview.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
    <p class="teaser-caption">
      This video provides a high-level overview of our VLS framework and its capabilities for steering pretrained robot policies in out-of-distribution scenarios.
    </p>
  </div>
</section>

<!-- Abstract -->
<section>
  <div class="container">
    <h2 class="section-title">
      <span class="icon"><i class="fas fa-align-left"></i></span>
      Abstract
    </h2>
    <div class="prose">
      <p>
        Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle,
        on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead,
        they expose a limitation of imitation learning under train–test shifts, where action generation is tightly coupled
        to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these
        failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively
        adapted at test time.
      </p>
      <p>
        We propose <strong>Vision–Language Steering (VLS)</strong>, a training-free framework for inference-time adaptation
        of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling
        process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation–language inputs
        without modifying policy parameters. By leveraging vision–language models to synthesize trajectory-differentiable reward
        functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements.
      </p>
    </div>

    <!-- Key Stats -->
    <div class="stats-grid">
      <div class="stat-card">
        <div class="stat-value">31%</div>
        <div class="stat-label">Improvement on CALVIN</div>
      </div>
      <div class="stat-card">
        <div class="stat-value">13%</div>
        <div class="stat-label">Gain on LIBERO-PRO</div>
      </div>
      <div class="stat-card">
        <div class="stat-value">0</div>
        <div class="stat-label">Training Required</div>
      </div>
    </div>
  </div>
</section>

<!-- Method -->
<section class="alt-bg">
  <div class="container">
    <h2 class="section-title">
      <span class="icon"><i class="fas fa-cogs"></i></span>
      Method Overview
    </h2>

    <figure class="method-figure">
      <img src="./static/images/Figure2.png" alt="VLS Pipeline">
      <figcaption>
        <strong>VLS Pipeline:</strong> Given RGB-D observation and language instruction, VLS grounds the input into task-relevant
        3D keypoints using SAM and DINOv2. A VLM generates stage-aware differentiable reward functions that guide the frozen base
        policy's denoising process through gradient-based refinement, RBF diversity, and Feynman–Kac resampling.
      </figcaption>
    </figure>

    <div class="components-grid">
      <div class="component-card">
        <div class="component-num">1</div>
        <h4>OOD Input Grounding & Reward Generation</h4>
        <p>
          VLS grounds out-of-distribution observation–language inputs into a compact geometric scaffold of task-relevant
          3D keypoints using SAM and DINOv2 features. A VLM decomposes the task into sequential stages and synthesizes
          differentiable, programmatic reward functions as PyTorch operations.
        </p>
      </div>
      <div class="component-card">
        <div class="component-num">2</div>
        <h4>Inference-Time Denoising Guidance</h4>
        <p>
          VLS steers sampling through three mechanisms: <strong>RBF-based diversity</strong> using repulsive forces,
          <strong>gradient-based refinement</strong> injecting reward gradients into denoising updates, and
          <strong>Feynman–Kac resampling</strong> that tilts the distribution toward high-reward regions.
        </p>
      </div>
      <div class="component-card">
        <div class="component-num">3</div>
        <h4>Closed-Loop Execution Control</h4>
        <p>
          To handle physical uncertainty, VLS employs adaptive guidance strength based on reward feedback and a
          <strong>Schmitt-trigger-based stage switching</strong> mechanism. This hysteresis-based approach avoids
          oscillatory behavior and enables stable multi-stage task execution.
        </p>
      </div>
    </div>
  </div>
</section>

<!-- CALVIN Rollouts -->
<section>
  <div class="container">
    <h2 class="section-title">
      <span class="icon"><i class="fas fa-robot"></i></span>
      CALVIN Rollouts
    </h2>
    <p class="prose">
      VLS enables robust manipulation across diverse tasks in the CALVIN benchmark. Below are example rollouts showing
      successful task completion for movable objects (colored cubes) and articulated parts (drawer, switch, button, door).
    </p>

    <h3 class="video-section-title">Movable Objects (Cubes)</h3>
    <div class="video-grid">
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/red.mp4" type="video/mp4">
        </video>
        <div class="video-label">Move Red Cube</div>
      </div>
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/blue.mp4" type="video/mp4">
        </video>
        <div class="video-label">Move Blue Cube</div>
      </div>
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/pink.mp4" type="video/mp4">
        </video>
        <div class="video-label">Move Pink Cube</div>
      </div>
    </div>

    <h3 class="video-section-title">Articulated Parts</h3>
    <div class="video-grid">
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/drawer_open.mp4" type="video/mp4">
        </video>
        <div class="video-label">Open Drawer</div>
      </div>
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/drawer_close.mp4" type="video/mp4">
        </video>
        <div class="video-label">Close Drawer</div>
      </div>
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/door_left.mp4" type="video/mp4">
        </video>
        <div class="video-label">Open Door Left</div>
      </div>
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/door_right.mp4" type="video/mp4">
        </video>
        <div class="video-label">Open Door Right</div>
      </div>
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/switch_on.mp4" type="video/mp4">
        </video>
        <div class="video-label">Turn On Switch</div>
      </div>
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/switch_off.mp4" type="video/mp4">
        </video>
        <div class="video-label">Turn Off Switch</div>
      </div>
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/button_on.mp4" type="video/mp4">
        </video>
        <div class="video-label">Press Button On</div>
      </div>
      <div class="video-card">
        <video autoplay muted loop playsinline>
          <source src="./static/videos/calvin/button_off.mp4" type="video/mp4">
        </video>
        <div class="video-label">Press Button Off</div>
      </div>
    </div>
  </div>
</section>

<!-- LIBERO Rollouts -->
<section class="alt-bg">
  <div class="container">
    <h2 class="section-title">
      <span class="icon"><i class="fas fa-tasks"></i></span>
      LIBERO-PRO Rollouts
    </h2>
    <p class="prose">
      VLS improves the frozen π-0.5 policy under both task perturbations (changed language instructions) and
      position perturbations (relocated objects) across LIBERO's four task suites: Goal, Spatial, Long (10), and Object.
    </p>

    <h3 class="video-section-title">Task Perturbation (Language OOD)</h3>
    <div class="video-grid">
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Goal Suite</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Spatial Suite</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Long (10) Suite</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Object Suite</div>
      </div>
    </div>

    <h3 class="video-section-title">Position Perturbation (Observation OOD)</h3>
    <div class="video-grid">
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Goal Suite</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Spatial Suite</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Long (10) Suite</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Object Suite</div>
      </div>
    </div>
  </div>
</section>

<!-- Real-World Rollouts -->
<section>
  <div class="container">
    <h2 class="section-title">
      <span class="icon"><i class="fas fa-hand-paper"></i></span>
      Real-World Rollouts (Franka Robot)
    </h2>
    <p class="prose">
      VLS enables robust real-world deployment on a Franka Emika robot in a kitchen environment with a DROID-style camera setup.
      We demonstrate both in-distribution tasks and out-of-distribution scenarios including appearance, position, and object shifts.
    </p>

    <h3 class="video-section-title">In-Distribution Tasks</h3>
    <div class="video-grid">
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Place orange on red plate</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Place orange on green plate</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Place banana on red plate</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Place orange on green plate (L2)</div>
      </div>
    </div>

    <h3 class="video-section-title">Out-of-Distribution Tasks</h3>
    <div class="video-grid">
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Appearance Shift: Yellow plate</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Position Shift: Swapped plates</div>
      </div>
      <div class="video-card">
        <div class="video-placeholder">Video coming soon</div>
        <div class="video-label">Object Shift: Place mug on plate</div>
      </div>
    </div>
  </div>
</section>

<!-- Quantitative Results -->
<section class="alt-bg">
  <div class="container">
    <h2 class="section-title">
      <span class="icon"><i class="fas fa-chart-bar"></i></span>
      Quantitative Results
    </h2>

    <!-- LIBERO-PRO Table -->
    <h3 style="font-size: 1.25rem; font-weight: 600; margin: 30px 0 16px;">LIBERO-PRO: Inference-Time Steering Is Necessary</h3>
    <p class="prose" style="margin-bottom: 20px;">
      We evaluate four leading VLA models under position and task perturbations. Despite strong in-distribution performance,
      pretrained VLAs fail under OOD conditions—demonstrating that inference-time steering is essential.
    </p>

    <div class="results-table-container">
      <table class="results-table">
        <thead>
          <tr>
            <th rowspan="2" class="method-name" style="vertical-align: middle;">Method</th>
            <th colspan="5" style="border-bottom: 1px solid var(--border);">Task Perturbation</th>
            <th colspan="5" style="border-bottom: 1px solid var(--border);">Position Perturbation</th>
            <th rowspan="2" style="vertical-align: middle;">Overall</th>
          </tr>
          <tr>
            <th>Goal</th>
            <th>Spatial</th>
            <th>10</th>
            <th>Object</th>
            <th>Avg.</th>
            <th>Goal</th>
            <th>Spatial</th>
            <th>10</th>
            <th>Object</th>
            <th>Avg.</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td class="method-name">OpenVLA</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
          </tr>
          <tr>
            <td class="method-name">π-0</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
            <td>0.00</td>
          </tr>
          <tr>
            <td class="method-name">π-0.5</td>
            <td>0.00</td>
            <td>1.00</td>
            <td>1.00</td>
            <td>1.00</td>
            <td>0.75</td>
            <td>38.00</td>
            <td>20.00</td>
            <td>8.00</td>
            <td>17.00</td>
            <td>20.75</td>
            <td>10.75</td>
          </tr>
          <tr>
            <td class="method-name">π-0.5 (LeRobot)</td>
            <td>12.00</td>
            <td>48.50</td>
            <td>21.50</td>
            <td>10.50</td>
            <td>23.13</td>
            <td>29.00</td>
            <td>41.00</td>
            <td>11.00</td>
            <td>16.00</td>
            <td>24.25</td>
            <td>23.69</td>
          </tr>
          <tr class="highlight-row">
            <td class="method-name">π-0.5 (LeRobot) + VLS</td>
            <td><strong>33.50</strong></td>
            <td><strong>54.00</strong></td>
            <td><strong>25.50</strong></td>
            <td><strong>41.00</strong></td>
            <td><strong>38.50</strong></td>
            <td><strong>38.00</strong></td>
            <td><strong>42.00</strong></td>
            <td><strong>15.50</strong></td>
            <td><strong>45.00</strong></td>
            <td><strong>35.13</strong></td>
            <td><strong>36.81</strong></td>
          </tr>
        </tbody>
      </table>
    </div>

    <!-- CALVIN Results -->
    <h3 style="font-size: 1.25rem; font-weight: 600; margin: 50px 0 16px;">CALVIN: VLS Outperforms Existing Steering Methods</h3>

    <figure class="full-figure">
      <img src="./static/images/CALVIN_result.png" alt="CALVIN Results">
      <figcaption>
        <strong>Steering methods comparison on CALVIN.</strong> Success rates for VLS (ours), DynaGuide, ITPS, and the base
        diffusion policy across movable objects (cubes) and articulated parts (drawer, switch, button, door).
        VLS achieves <strong>94% average on movable objects</strong> (7.4× over base policy) and <strong>87% on articulated
        parts</strong> (9.6× boost), outperforming prior steering methods by <strong>15–25 percentage points</strong>.
        Error bars show standard deviation over 600 episodes per task.
      </figcaption>
    </figure>

    <!-- Ablation -->
    <h3 style="font-size: 1.25rem; font-weight: 600; margin: 50px 0 16px;">Ablation Study</h3>

    <figure class="full-figure">
      <img src="./static/images/ablation_results.png" alt="Ablation Study">
      <figcaption>
        <strong>(Left) Ablation of VLS components</strong> (50 episodes per task). We compare Full VLS (gradient guidance +
        FK steering + RBF diversity) against variants that remove each component. Removing gradient guidance causes severe
        performance collapse, confirming it as the primary driver of VLS's effectiveness.
        <strong>(Right) Scaling with sample batch size</strong> on door_left task. Larger batch sizes improve performance
        at the cost of inference time, illustrating a practical compute–performance tradeoff.
      </figcaption>
    </figure>
  </div>
</section>

<!-- Real-World Deployment Results -->
<section>
  <div class="container">
    <h2 class="section-title">
      <span class="icon"><i class="fas fa-flask"></i></span>
      Real-World Deployment
    </h2>

    <p class="prose">
      We evaluate VLS on a <strong>Franka Emika robot</strong> with a Robotiq gripper in a kitchen environment.
      The setup uses a DROID-style camera configuration with a Zed mini (wrist) and Zed 2 (side) cameras.
    </p>

    <figure class="full-figure" style="margin-top: 30px;">
      <img src="./static/images/pi05_results.png" alt="Real-World Deployment Results">
      <figcaption>
        <strong>(Left) In-Distribution Tasks:</strong> Level 1 requires placing an orange onto a specified plate (red or green).
        Level 2 introduces a banana, requiring sequential object and plate selection.
        <strong>(Right) Out-of-Distribution Tasks:</strong> (1) <em>Appearance shift</em>: replacing red/green plate with unseen
        yellow plate; (2) <em>Position shift</em>: swapping plate locations; (3) <em>Object shift</em>: replacing banana with
        a never-before-seen mug. Each task evaluated over 20 trials.
      </figcaption>
    </figure>

    <ul class="findings-list" style="margin-top: 30px;">
      <li>
        <span><strong>In-distribution:</strong> VLS achieves <strong>69% average success rate</strong>, outperforming the
        frozen π-0.5 baseline by <strong>19%</strong>.</span>
      </li>
      <li>
        <span><strong>Out-of-distribution:</strong> The baseline's performance degrades sharply under OOD conditions, while
        VLS maintains stable execution across all shift types.</span>
      </li>
      <li>
        <span><strong>Novel object handling:</strong> In the most challenging case (replacing banana with an unseen mug),
        the baseline achieves <strong>0% success</strong> while VLS succeeds in <strong>40% of trials</strong>.</span>
      </li>
    </ul>
  </div>
</section>

<!-- Key Findings -->
<section class="alt-bg">
  <div class="container">
    <h2 class="section-title">
      <span class="icon"><i class="fas fa-lightbulb"></i></span>
      Key Findings
    </h2>

    <ul class="findings-list">
      <li>
        <span><strong>Inference-time steering is necessary:</strong> Pretrained VLAs, despite strong in-distribution
        performance, fail under OOD conditions. Post-training on robot data entangles spatial reasoning with specific
        training contexts.</span>
      </li>
      <li>
        <span><strong>VLS outperforms heuristic guidance:</strong> Unlike DynaGuide (DINO-feature heuristics) and ITPS
        (predefined guidance functions), VLS conditions guidance directly on observation–language inputs, enabling precise
        steering under spatial variability.</span>
      </li>
      <li>
        <span><strong>Both gradient-based and gradient-free components matter:</strong> Gradient guidance provides dense
        refinement, while FK resampling and RBF diversity improve sample efficiency and prevent premature mode collapse.</span>
      </li>
      <li>
        <span><strong>Practical compute–performance tradeoff:</strong> Batch size can be tuned for deployment—smaller for
        lower latency, larger for maximum robustness.</span>
      </li>
    </ul>
  </div>
</section>

<!-- Conclusion -->
<section>
  <div class="container">
    <h2 class="section-title">
      <span class="icon"><i class="fas fa-flag-checkered"></i></span>
      Conclusion
    </h2>

    <div class="prose">
      <p>
        We propose <strong>VLS</strong>, a training-free framework that guides pretrained robotic policies using differentiable
        rewards generated by Vision–Language Models, addressing the challenge of policy deployment in out-of-distribution
        scenarios. By treating adaptation as an inference-time control problem rather than a retraining problem, VLS enables
        frozen diffusion and flow-matching policies to execute reliably under spatial and semantic shifts that would otherwise
        cause failure.
      </p>
      <p>
        Experiments demonstrate that VLS significantly outperforms existing methods in both simulation (<strong>31%
        improvement on CALVIN</strong>, <strong>13% on LIBERO-PRO</strong>) and real-world tasks on a Franka robot.
      </p>
    </div>

    <h3 style="font-size: 1.1rem; font-weight: 600; margin: 30px 0 16px;">Limitations and Future Work</h3>
    <ul class="findings-list">
      <li>
        <span><strong>Computational latency:</strong> Batch sampling, MCMC runs, and FK resampling introduce inference
        overhead that may limit real-time applications.</span>
      </li>
      <li>
        <span><strong>VLM capability dependence:</strong> Steering quality depends on the VLM's ability to correctly
        decompose tasks and generate appropriate reward functions.</span>
      </li>
      <li>
        <span><strong>Future directions:</strong> Progress-aware reward signal generation and optimizing computational
        efficiency during inference.</span>
      </li>
    </ul>
  </div>
</section>

<!-- BibTeX -->
<section class="alt-bg">
  <div class="container">
    <h2 class="section-title">
      <span class="icon"><i class="fas fa-quote-left"></i></span>
      BibTeX
    </h2>

    <div class="bibtex-box">
      <pre><code>@inproceedings{anonymous2026vls,
  title     = {VLS: Steering Pretrained Robot Policies via Vision–Language Models},
  author    = {Author One and Author Two and Author Three and Author Four and Author Five},
  booktitle = {Conference on Robotics},
  year      = {2026}
}</code></pre>
    </div>
  </div>
</section>

<!-- Footer -->
<footer>
  <div class="container">
    <p>
      Template inspired by <a href="https://github.com/nerfies/nerfies.github.io" target="_blank">Nerfies</a>.
    </p>
  </div>
</footer>

</body>
</html>
